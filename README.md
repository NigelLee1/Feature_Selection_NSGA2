# Feature_Selection_NSGA2
minimize the number of features and classification errors by using NSGA-II

Feature selection is an important task in machine learning and data mining, which involves identifying a subset of relevant features from a large set of features to improve the performance of a model. NSGA2 as a Multi-objective optimization for feature selection has gained increasing attention in recent years due to the evolutionary nature that performs selection, crossover, and mutation operations to generate new offspring individuals with higher fitness compared to the previous generation. In this experiment, we apply this algorithm to a low-scale dataset and a large-scale dataset, namely “Hillvally” and “Musk1” respectively, the min-min optimization problem was used to solve two objectives: (1) minimizing the number of selected features, and (2) minimizing the classification error on training set of a KNN classifier on the selected features. We evaluate the performance of NSGA2 based on non-dominated sorting rankings and hypervolume of the Pareto fronts.

Methodology

In the project, the third-party package “pymoo” was used for implementing the NSGA2 algorithms, whereby the min-min multi-objective optimization problem was solved efficiently and effectively. Normalization was a crucial step to scaling all values from 0 to 1, not only essential for balancing features’ weights, but also useful for calculating the hypervolume with a reference point (1, 1). In the evolution process of NSGA2, minimization of classification error on training set was an objective not minimizing classification error on testing set. However, after the optimization, minimum classification error on test samples as higher-level information was used to choose optimal solution from the resultant Pareto front. In the case that all bits in a vector were set to zero that is no features were selected for the KNN model, we simply return the training error as 100%.
